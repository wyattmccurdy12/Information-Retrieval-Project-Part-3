import argparse
import json
import re
import pandas as pd
from tqdm import tqdm
import transformers
from transformers import pipeline
from sentence_transformers import SentenceTransformer, util
import torch
import os
import numpy as np

class QXRetriever:

    MODEL_ID = "meta-llama/Llama-3.2-1B"

    def __init__(self, bi_encoder_model='sentence-transformers/multi-qa-mpnet-base-dot-v1'):
        """
        Initializes the QXRetriever instance and sets up the LLaMA model pipeline and bi-encoder.
        """
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self.initialize_llama()
        self.bi_encoder = SentenceTransformer(bi_encoder_model).to(self.device)
        self.document_embeddings = {}

    def initialize_llama(self):
        """
        Initialize the LLaMA model pipeline.

        Returns:
            pipeline: The initialized LLaMA model pipeline.
        """
        pipe = pipeline(
            "text-generation", 
            model=self.MODEL_ID, 
            torch_dtype=torch.bfloat16, 
            device_map="auto",
            max_new_tokens=100  # Set max_new_tokens to 100
        )
        return pipe

    def encode_documents(self, documents):
        """
        Encode the documents using the bi-encoder and store their embeddings.

        Args:
            documents (dict): A dictionary where keys are document IDs and values are document texts.
        """
        for doc_id, text in tqdm(documents.items(), desc="Encoding documents"):
            text_tensor = self.bi_encoder.encode(text, convert_to_tensor=True, device=self.device)
            self.document_embeddings[doc_id] = text_tensor.cpu().numpy()

        # Save document embeddings to a file
        np.savez('document_embeddings.npz', **self.document_embeddings)

    def load_document_embeddings(self, filepath):
        """
        Load document embeddings from a file.

        Args:
            filepath (str): The path to the file containing document embeddings.
        """
        self.document_embeddings = np.load(filepath)

    def retrieve_documents(self, expanded_query, documents, top_k=100):
        """
        Retrieve documents that match the expanded query using bi-encoder.

        Args:
            expanded_query (str): The query string with expanded terms.
            documents (dict): A dictionary where keys are document IDs and values are document texts.
            top_k (int): The number of top documents to retrieve.

        Returns:
            list: A list of dictionaries, each containing 'Id', 'Text', and 'Score' of documents that match the expanded query.
        """
        query_embedding = self.bi_encoder.encode(expanded_query, convert_to_tensor=True, device=self.device)
        results = []
        for doc_id, doc_embedding in self.document_embeddings.items():
            score = util.pytorch_cos_sim(query_embedding, torch.tensor(doc_embedding).to(self.device)).item()
            results.append({'Id': doc_id, 'Text': documents[doc_id], 'Score': score})
        results = sorted(results, key=lambda x: x['Score'], reverse=True)[:top_k]
        return results

    def gen_with_llama(self, text_input):
        """
        Expand text input using LLaMA 3.2 1B.

        This method takes a text input and generates expanded text using the LLaMA model.
        If a ValueError occurs during text generation, the original text input is returned.

        Args:
            text_input (str): The input text to be expanded.

        Returns:
            str: The expanded text generated by the LLaMA model, or the original text input if an error occurs.
        """
        try:
            gen_text = self.model(text_input, max_new_tokens=100)[0]['generated_text']
        except ValueError as e:
            print(f"ValueError: {e}")
            return text_input  # Return the original text if a ValueError occurs
        return gen_text

    def expand_query_title(self, topic):
        """
        Expand the title field of the topics JSON and return a copy of the JSON exactly identical to the original but with an expanded title.

        Args:
            topic (dict): The JSON formatted dictionary containing queries with Id, Title, and Body fields.

        Returns:
            dict: The updated topic with an expanded title.
        """
        title = topic['Title']
        expanded_title = self.gen_with_llama(title)
        topic['Title'] = expanded_title
        return topic

    @staticmethod
    def load_queries(filepath):
        """
        Load queries from a JSON file.

        Args:
            filepath (str): The path to the JSON file containing queries.

        Returns:
            list: A list of queries loaded from the JSON file.
        """
        with open(filepath, 'r') as file:
            queries = json.load(file)
        return queries

    @staticmethod
    def load_documents(filepath):
        """
        Load documents from a JSON file.

        Args:
            filepath (str): The path to the JSON file containing documents.

        Returns:
            dict: A dictionary of documents loaded from the JSON file.
        """
        with open(filepath, 'r') as file:
            documents = json.load(file)
        return documents

    @staticmethod
    def remove_html_tags(text):
        """
        Remove HTML tags from a string.

        Args:
            text (str): The input string containing HTML tags.

        Returns:
            str: The cleaned string with HTML tags removed.
        """
        clean = re.compile('<.*?>')
        return re.sub(clean, '', text)
    

def main():
    parser = argparse.ArgumentParser(description='Process some integers.')
    parser.add_argument('-q', '--queries_file', default='topics_2.json', help='Path to the queries JSON file')
    parser.add_argument('-d', '--documents_file', default='Answers.json', help='Path to the documents JSON file')
    parser.add_argument('-be', '--bi_encoder', default='sentence-transformers/multi-qa-mpnet-base-dot-v1', help='Bi-encoder model string')
    parser.add_argument('-o', '--output_dir', default='data/outputs/', help='Output directory for expanded queries')
    parser.add_argument('--write-expanded', action='store_true', default=True, help='Write expanded queries to a JSON file')
    parser.add_argument('--load-embeddings', action='store_true', default=True, help='Load document embeddings from file')
    args = parser.parse_args()

    processed_query_ids = set()
    expanded_queries_file = os.path.join(args.output_dir, f"{os.path.splitext(os.path.basename(args.queries_file))[0]}_expanded.json")
    processed_ids_file = os.path.join(args.output_dir, f"{os.path.splitext(os.path.basename(args.queries_file))[0]}_processed_ids.json")

    try:
        # Load data
        print("Loading queries...")
        queries = QXRetriever.load_queries(f"data/inputs/{args.queries_file}")
        print("Loading documents...")
        documents = QXRetriever.load_documents(f"data/inputs/{args.documents_file}")
        
        # Initialize QXRetriever
        print("Initializing QXRetriever...")
        retriever = QXRetriever(bi_encoder_model=args.bi_encoder)
        
        # Process and encode documents
        if args.load_embeddings and os.path.exists('document_embeddings.npz'):
            print("Loading document embeddings from file...")
            retriever.load_document_embeddings('document_embeddings.npz')
        else:
            print("Processing and encoding documents...")
            processed_documents = {}
            for doc in tqdm(documents, desc="Processing documents"):
                doc_id = doc['Id']
                text = QXRetriever.remove_html_tags(doc['Text'])
                processed_documents[doc_id] = text
            
            # Encode documents
            retriever.encode_documents(processed_documents)
        
        # Load processed query IDs if they exist
        if os.path.exists(processed_ids_file):
            with open(processed_ids_file, 'r') as f:
                processed_query_ids = set(json.load(f))

        # Process queries
        print("Processing queries...")
        expanded_queries = []

        if not args.write_expanded and os.path.exists(expanded_queries_file):
            print(f"Loading expanded queries from {expanded_queries_file}...")
            with open(expanded_queries_file, 'r') as f:
                expanded_queries = json.load(f)
        else:
            with open(expanded_queries_file, 'w') as f:
                f.write('[')  # Start of JSON array
                first_entry = True
                for query in tqdm(queries, desc="Processing queries"):
                    query_id = query['Id']
                    if query_id in processed_query_ids:
                        continue
                    expanded_query = retriever.expand_query_title(query)
                    expanded_queries.append(expanded_query)
                    processed_query_ids.add(query_id)
                    if not first_entry:
                        f.write(',\n')
                    json.dump(expanded_query, f, indent=4)
                    first_entry = False
                f.write(']')  # End of JSON array

            # Save processed query IDs
            with open(processed_ids_file, 'w') as f:
                json.dump(list(processed_query_ids), f)
        
        # Write expanded queries to a JSON file if the flag is set
        if args.write_expanded:
            with open(expanded_queries_file, 'w') as f:
                json.dump(expanded_queries, f, indent=4)
            print(f"Expanded queries written to {expanded_queries_file}")

    except Exception as e:
        # Save processed query IDs in case of an exception
        with open(processed_ids_file, 'w') as f:
            json.dump(list(processed_query_ids), f)
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    main()